# Deep-learning-assignment1
# CS6910 Assignment 1


# Team members: REDDY JOSHNA MANOJ (EE21S113),AKSHAY G RAO(CS21S002)

# type the function name in search(ctrl+f)in the notebook to go to particular code
# to execute the best accurate model that we got please run the following command in google colab to get the results runBestModelAndPlotConfusionMatrix() or uncomment the the last line in the code
Complete code is in the Assignment_1.ipynb
1.Code Starts with importing libraries and intailization of wandb
# Question 1 (https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=vhblbkxZZTB3)
Next code to the Question 1 starts in the third cell of notebook program, reads the data from `keras.datasets`, picks one example from each class and logs the same to `wandb`.

# Question 2 (run function at https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=MqcfAefoqcty&line=3&uniqifier=1  by specifying hyper-parameters)
The neural network is implemented by the  class ANNModel()(type the function name in search to go to particular code)
Executing feed forward neural network model with setting proper hyper parameters , following is the method
 model = runModelOnDataWithHP(config=None,x_train=x_train,x_valid=x_valid,y_train=y_train,y_valid=y_valid,activation_per_layer='tanh',epochs=10,hidden_layer_size=3,learning_rate=0.0001,num_neuron_per_hidden_layer=128,optimizer='nesterov',batch_size=128)
 **epochs**  
    The Batch Size and epochs is passed as an integer that determines the size of the mini batch to be taken into consideration per epoch.
**hidden_layer_size** - integer
**num_neuron_per_hidden_layer**-Integer

# Question 3 
**Backpropogation**
complete Backpropogation Algorthim with choice of gradient update(cross entropy /MSE) is written in the function:
back_prop(self,y_pred,y_train) and computeLossGradient(self,y_pred,y_actual,loss)
**optimizers**
All the optimizers codes : are written in  the method updateParameters(self,optimizer='vanilla',learning_rate=1)
(sgd,momentum based gradient descent,nesterov accelerated gradient descent,rmsprop,adam,nadam)

# Question 4,5,6 (code at https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=C_kg19j61M5B&line=3&uniqifier=1 )
All the sweeps and runs are done by using the function hpTuningWithWandb(config=None)
Detail explaination is given in the wandb report that is shared in gradescope

# Question 7 ( code at https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=yuYg7YhZJ-bt&line=5&uniqifier=1 )
Best  model with best hyperparameters are run on test data to get test accuracy and confusion matrix
the function and  run command is  runBestModelAndPlotConfusionMatrix()

# Question 8 ( change loss function at https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=oISiX--9deq5&line=7&uniqifier=1 )
Best model with hyperparameters that is giving  high accuracy is run by changing one hyperparameter ( "gradient loss" from cross entropy to MSE(Mean Square error))
plots are generated by wandb and compared (please refer the report)


# Question 10 (starts from https://colab.research.google.com/drive/1G1UK5v_L3mveFipIQPf_OHi2BjevAu9A#scrollTo=ilK0dRXMeeQ8&line=7&uniqifier=1 )
The function name used for HP tuning(3 variations) is "mnistHpTuningWithWandb". For convenience, we have named the block as Question 10 and code following this placeholder has code for question 10


